#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Dec 17 08:54:36 2024

This code used the data generated by "numerical_main_2.py" to produce Figure 2, Tables 1 and 2.

@author: leonliu
"""

import numpy as np
import time
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd
import scipy.stats as s
import scipy
import warnings
import statsmodels.api as sm
from scipy.stats import t
from scipy.special import gammaincc, gamma,gammainccinv,lambertw


def plot_results_all_a(rep, sample_sizes, Numeric_Result, a_list,weibull_color='#4682B4',log_scale=1,save_path=0):
    # Create an empty DataFrame
    data = pd.DataFrame(columns=['qt_err', 'cdf_err','max_err_pt' ,'distribution', 'sample size'])
    
    # Initialize lists to store data
    qt_err_list = []
    cdf_err_list = []
    max_err_pt_list = []
    distribution_list = []
    sample_size_list = []
    
    # Populate lists with data from Numeric_Result
    for ss in sample_sizes:
        for a in a_list:
            qt_err_list += Numeric_Result[ss, 'weibull',a, 'qt_err']
            cdf_err_list += Numeric_Result[ss, 'weibull',a, 'cdf_err']
            max_err_pt_list += Numeric_Result[ss, 'weibull',a, 'max_err_pt']
            distribution_list += ['weibull: '+str(a)] * rep
            
        qt_err_list += Numeric_Result[ss, 'uniform', 'qt_err']
        cdf_err_list += Numeric_Result[ss, 'uniform', 'cdf_err']
        max_err_pt_list += Numeric_Result[ss, 'uniform', 'max_err_pt']
        distribution_list += ['uniform'] * rep

        sample_size_list += [ss] * (len(a_list)+1) * rep

    # Populate the DataFrame with the lists
    data['qt_err'] = qt_err_list
    data['cdf_err'] = cdf_err_list
    data['max_err_pt'] = max_err_pt_list
    data['distribution'] = distribution_list
    data['sample size'] = sample_size_list
    
    legend_label_mapping = {f'weibull: {a}': f'Weibull: {a}' for a in a_list}
    
    legend_label_mapping['uniform'] = 'Uniform'
    # Replace 'distribution' column values with renamed labels
    data['distribution'] = data['distribution'].replace(legend_label_mapping)
    # Define the new hue_order for consistent ordering
    hue_order = [f'Weibull: {a}' for a in a_list]
    hue_order.append('Uniform')
    
    sns.set(style="whitegrid")
    palette =  [weibull_color] * len(a_list) + ['r'] 

    # Plot mean value with 95% CI for 'qt_err'
    weibull_markers = ['^', 's', 'P', 'o', 'D', 'v', '*']
    weibull_markers = weibull_markers[:len(a_list)]
    fig, ax = plt.subplots(1)
    sns.lineplot(
        x='sample size',
        y='qt_err',
        hue='distribution',
        data=data,
        palette=palette,
        estimator=np.mean,
        ci=95,
        ax=ax,
        err_style="band",
        style='distribution',
        dashes=[(2, 2)] * len(a_list) + [(1, 0)],
        markers= weibull_markers+['X'],
        markersize=10
    )
    if log_scale == 1:
        ax.set_xscale('log')  # Set x-axis to logarithmic scale
        ax.set_xlabel('Sample size (log scale)', fontsize=16)
    else:
        ax.set_xlabel('Sample size', fontsize=16)
    ax.set_ylabel('Moderated Quantile Error', fontsize=16)
    plt.ylim([-0.05,1.05])
    ax.tick_params(axis='both', labelsize=14)
    #ax.set_xticks(sample_sizes)
    handles, labels = ax.get_legend_handles_labels()
    #ax.legend(handles, labels, fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.legend(handles, labels, fontsize=12, loc='upper right')
    if save_path != 0:
        plt.savefig(save_path, format='png', dpi=600, bbox_inches='tight')
    
    fig, ax = plt.subplots(1)
    sns.lineplot(
        x='sample size',
        y='max_err_pt',
        hue='distribution',
        data=data,
        palette=palette,
        estimator=np.mean,
        ci=95,
        ax=ax,
        err_style="band",
        style='distribution',
        dashes=[(2, 2)] * len(a_list) + [(1, 0)],
        markers= weibull_markers+['X'],
        markersize=10
    )
    if log_scale == 1:
        ax.set_xscale('log')  # Set x-axis to logarithmic scale
        ax.set_xlabel('Sample size (log scale)', fontsize=16)
    else:
        ax.set_xlabel('Sample size', fontsize=16)
    ax.set_ylabel('Supremum Quantile Error Point', fontsize=16)
    ax.tick_params(axis='both', labelsize=14)
    plt.ylim([-0.05,1.05])
    #ax.set_xticks(sample_sizes)
    handles, labels = ax.get_legend_handles_labels()
    #ax.legend(handles, labels, fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')
    ax.legend(handles, labels, fontsize=12, loc='upper right')
    #if save_path != 0:
        #plt.savefig(save_path, format='png', dpi=600, bbox_inches='tight')
    plt.show()

if __name__ == '__main__':
    mean = 1
    rep = 100
    Numeric_Result = np.load('Numeric_Result_qt_cdf_err_range100w_rep100_a_all.npy',allow_pickle=True).item()
    sample_sizes = [10,50,100,500,1000,5000,10000,50000,100000,500000,1000000]
    a_list = [0.1,0.125,0.15,0.175,0.2,0.75,1,1.25,2,4,8,16,32]
    a_list_1 = [0.1,0.125,0.15,0.175,0.2,0.75,1]
    a_list_2 =[1.25,2,4,8,16,32]
    plot_results_all_a(rep, sample_sizes, Numeric_Result, a_list_1,log_scale=1)
    plot_results_all_a(rep, sample_sizes, Numeric_Result, a_list_2,log_scale=1)
    #### regression analysis #######
    log_plot = 0
    print('sample_sizes:',sample_sizes)
    
    def transform_err(epsilon,a,mean):
         scale = mean/gamma(1 + 1/a)
         p = epsilon*a/scale
         v = gammainccinv(1/a, p/gamma(1/a))
         y = v**(1/a)*scale
         return np.log(1/s.weibull_min.cdf(y, a, scale=scale)),np.exp(-v)
    
    def transform_err_Lambert(epsilon,a,mean):
         a_inv = 1/a
         scale = mean/gamma(1 + 1/a)
         if a == 1:
             v_lambert= -np.log(a*epsilon/scale) # -a*W(\eps^{1/a}/a) -> -ln(\eps) as a->0
         else:
             v_lambert = -(a_inv-1)*lambertw(-(a*epsilon/scale)**(1/(a_inv-1))/(a_inv-1), k=-1).real
         return np.exp(-v_lambert)
     
    def transform_err_Lambert_expand(eps,k,mean):
        k_inv = 1/k
        scale = mean/gamma(1 + k_inv)
        def Lambert_expand(x):
            return np.log(-x) - np.log(-np.log(-x))
        if k ==1:
            v_lambert_expand = -np.log(k*eps/scale) 
        else:
            v_lambert_expand = -(k_inv-1)*Lambert_expand(-(k*eps/scale)**(1/(k_inv-1))/(k_inv-1))
        return np.exp(-v_lambert_expand)
    
    def transform_sample_size(ss,k,mean):
        scale = mean/gamma(1 + 1/k)
        return gammaincc(1/k, np.log(ss))*gamma(1/k)*scale/k
    
    col_names = ['weibull-'+str(a) for a in a_list ]+['trans_weibull-'+str(a) for a in a_list ]+['trans_weibull_lam-'+str(a) for a in a_list ] + ['sample size']+['trans sample size']
    
    err_reg_data = pd.DataFrame(columns=col_names)
    
    sample_size_list = []
    trans_sample_size_list = []
    
    weibull_err_list = {}
    trans_weibull_err_list = {}
    trans_weibull_err_lam_list = {}
    trans_weibull_err_lam_expand_list = {}
    uniform_err_list = []
    for a in a_list:
        weibull_err_list[a] = []
        trans_weibull_err_list[a] = []
        trans_weibull_err_lam_list[a] = []
        trans_weibull_err_lam_expand_list[a] = []
        
    for ss in sample_sizes:
        uniform_err = Numeric_Result[ss,'uniform','qt_err']
        rep = len(uniform_err)
        sample_size_list += [ss]*rep
        trans_sample_size_list += [transform_sample_size(ss,a,mean)]*rep
        uniform_err_list += uniform_err
        
        for a in a_list:
            weibull_err = Numeric_Result[ss,'weibull',a,'qt_err']
            trans_weibull_err = [np.log(transform_err(x,a,mean)[0]) for x in weibull_err]
            trans_weibull_lam_err = [np.log(transform_err_Lambert(x,a,mean)) for x in weibull_err]
            trans_weibull_lam_expand_err = [np.log(transform_err_Lambert_expand(x,a,mean)) for x in weibull_err]
            #trans_weibull_err = [transform_err_Lambert_pre(x,a,mean) for x in weibull_err]
            weibull_err_list[a] += weibull_err
            trans_weibull_err_list[a] += trans_weibull_err
            trans_weibull_err_lam_list[a] += trans_weibull_lam_err
            trans_weibull_err_lam_expand_list[a] += trans_weibull_lam_expand_err
    
    err_reg_data['sample size'] = sample_size_list
    err_reg_data['trans sample size'] = trans_sample_size_list
    err_reg_data['uniform'] = uniform_err_list
    for a in a_list:
        err_reg_data['weibull-'+str(a)] = weibull_err_list[a]
        err_reg_data['trans_weibull-'+str(a)] = trans_weibull_err_list[a]
        err_reg_data['trans_weibull_lam-'+str(a)] = trans_weibull_err_lam_list[a]
        err_reg_data['trans_weibull_lam_expand-'+str(a)] = trans_weibull_err_lam_expand_list[a]
        
    ### hypothesis testing
    
    distribution_list = ['weibull-'+str(a) for a in a_list] + ["uniform"]
    for distribution in distribution_list:
        Y = err_reg_data['sample size']
        X = err_reg_data[distribution]
        log_X = np.log(X)
        log_Y = np.log(Y)
        log_Y_with_const = sm.add_constant(log_Y)
        model = sm.OLS(log_X, log_Y_with_const).fit()
        #model = sm.OLS(X, log_Y_with_const).fit()
        t_stat = (model.params[1] + 0.5) / model.bse[1]  # t-statistic for the null
        df_resid = model.df_resid  # Degrees of freedom
        p_value = 2 * (1 - t.cdf(abs(t_stat), df=df_resid))  # Two-tailed p-value
        print('---------------------------------')
        print(distribution+": log(error) VS log(sample)")
        print(f"sample size coeff: {model.params[1]}")
        print(f"H0: β1 = -0.5; p-value = {p_value}")
        
        #print(model.summary())
        if log_plot == 1:
            plt.figure(figsize=(8, 6))
            plt.scatter(log_Y, log_X, label="Data points", color="blue", alpha=0.7)
            fitted_X = model.predict(log_Y_with_const)
            plt.plot(log_Y, fitted_X, label="Fitted model", color="red", linestyle="--")
            plt.ylim([-8,0.5])
            plt.xlabel("log(sample size)")
            plt.ylabel("log(quantile error):"+distribution)
            plt.legend()
            plt.grid()
            plt.show()
    
    print('*****************************************')
    distribution_list = ['trans_weibull-'+str(a) for a in a_list] #+ ['trans_weibull_lam-'+str(a) for a in a_list]+ ['trans_weibull_lam_expand-'+str(a) for a in a_list]
    for distribution in distribution_list:
        Y = err_reg_data['sample size']
        X = err_reg_data[distribution]
        #log_X = np.log(X)
        log_Y = np.log(Y)
        log_Y_with_const = sm.add_constant(log_Y)
        #model = sm.OLS(log_X, log_Y_with_const).fit()
        model = sm.OLS(X, log_Y_with_const).fit()
        t_stat = (model.params[1] + 1) / model.bse[1]  # t-statistic for the null
        df_resid = model.df_resid  # Degrees of freedom
        p_value = 2 * (1 - t.cdf(abs(t_stat), df=df_resid))  # Two-tailed p-value
        print('---------------------------------')
        print(distribution+" VS log(sample)")
        print(f"sample size coeff: {model.params[1]}")
        print(f"intercept: {model.params[0]}")
        print(f"H0: β1 = -1; p-value = {p_value}")
    

        
